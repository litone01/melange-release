{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2E Eval Notebook\n",
    "This notebook takes as input: 1) dataset, 2) request rate, and 3) GPU Max tput profiling, and gives as output the GPU selection across all solver policies. \n",
    "\n",
    "The output is used to set up the online load-balancing evals to confirm SLO attainment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract request size distribution from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from transformers import LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained('hf-internal-testing/llama-tokenizer')\n",
    "\n",
    "def get_dataset(dataset_name):\n",
    "  dataset = load_dataset(dataset_name)\n",
    "  print(f'Dataset size: {len(dataset[\"train\"])}')\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def get_token_dataset(dataset):\n",
    "\n",
    "  # Get token lengths of conversations \n",
    "  # [(from, token length), ...]\n",
    "  token_dataset = []\n",
    "\n",
    "  print('Computing token lengths of {dataset_name}')\n",
    "  for i in range(1000):\n",
    "  # for i in range(len(dataset['train'])):\n",
    "    conversation = dataset['train'][i]['conversation']\n",
    "    if i % 100 == 0:\n",
    "      clear_output(wait=True)\n",
    "      print(f'Progress: {round(100 * i/len(dataset[\"train\"]), 2)}%')\n",
    "    token_dataset.append([])\n",
    "    for j in range(len(conversation)):\n",
    "      chat = conversation[j]\n",
    "      token_dataset[i].append((chat['role'], len(tokenizer(chat['content']).input_ids)))\n",
    "  return token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of input and output lenghths of LLM requests\n",
    "\n",
    "def get_chat_lens(token_dataset):\n",
    "\n",
    "  inlens = []\n",
    "  outlens = []\n",
    "\n",
    "  for convo in token_dataset:\n",
    "    convo_token_total = 0\n",
    "    for i in range(len(convo)):\n",
    "      author, chat_tokens = convo[i]\n",
    "      convo_token_total += chat_tokens\n",
    "      if author == 'user' and i + 1 < len(convo) and convo[i+1][0] == 'assistant':\n",
    "        inlens.append(convo_token_total)\n",
    "        outlens.append(convo[i+1][1])\n",
    "  print(f'Num chats: {len(inlens)}')\n",
    "  return inlens, outlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_distribution(inlens, outlens):\n",
    "  print(f'Inlen distribution:')\n",
    "  print('50p:', np.percentile(inlens, 50))\n",
    "  print('90p:', np.percentile(inlens, 90))\n",
    "  print('99p:', np.percentile(inlens, 99))\n",
    "  print(f'Max: {max(inlens)}')\n",
    "  print(f'Outlen distribution:')\n",
    "  print('50p:', np.percentile(outlens, 50))\n",
    "  print('90p:', np.percentile(outlens, 90))\n",
    "  print('99p:', np.percentile(outlens, 99))\n",
    "  print(f'Max: {max(outlens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def display_distribution(inlens, outlens):\n",
    "\n",
    "  regions = [0, 25, 100, 250, 500, 1000, 2000]\n",
    "  regions_txt = ['25', '100', '250', '500', '1000', '2000']\n",
    "\n",
    "  histogram, _, _ = np.histogram2d(inlens, outlens, bins=(regions, regions))\n",
    "\n",
    "\n",
    "  num_datapoints = sum([sum(x) for x in histogram])\n",
    "  histogram = histogram.T\n",
    "  histogram = histogram[::-1]\n",
    "  for i in range(len(histogram)):\n",
    "    for j in range(len(histogram[i])):\n",
    "      histogram[i][j] = round(histogram[i][j] / num_datapoints * 100, 2)\n",
    "\n",
    "\n",
    "  plt.figure(figsize=(8, 5))\n",
    "\n",
    "  cmap = sns.color_palette('flare', as_cmap=True)\n",
    "  heatmap = sns.heatmap(histogram, cmap=cmap, vmax=6.6, vmin=0, center = 3.3,\n",
    "              square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True,\n",
    "              xticklabels=regions_txt, yticklabels=regions_txt[::-1])\n",
    "\n",
    "  for text in heatmap.texts:\n",
    "      # text.set_size(18)\n",
    "      text.set_text(f\"{float(text.get_text())}%\")\n",
    "\n",
    "\n",
    "  # Rotate the labels on the y-axis\n",
    "  plt.yticks(rotation=0)\n",
    "  plt.tick_params(axis='y', labelsize=14)\n",
    "  plt.tick_params(axis='x', labelsize=14)\n",
    "  plt.xlabel('Prompt len (tokens)', fontsize=18)\n",
    "  plt.ylabel('Output len (tokens)', fontsize=18)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "  print(histogram)\n",
    "\n",
    "  arena_dataset = [x for x in inlens if x < 4096]\n",
    "  df = pd.DataFrame({\n",
    "      'Prompt Length': arena_dataset,\n",
    "      'Model': ['Arena']*len(arena_dataset)\n",
    "  })\n",
    "\n",
    "  sns.set(style=\"whitegrid\")\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  sns.histplot(data=df, x='Prompt Length', hue='Model', element='step', stat='count', common_norm=False, bins=50, alpha=0.5)\n",
    "\n",
    "  plt.xlabel('Prompt Length')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.title('Comparison of Output Lengths')\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "  arena_dataset = [x for x in outlens if x < 2048]\n",
    "\n",
    "\n",
    "  # Create a DataFrame\n",
    "  df = pd.DataFrame({\n",
    "      'Response Length': arena_dataset,\n",
    "      'Model': ['Arena']*len(arena_dataset)\n",
    "  })\n",
    "\n",
    "  # Create the plot with Seaborn\n",
    "  sns.set(style=\"whitegrid\")\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  sns.histplot(data=df, x='Response Length', hue='Model', element='step', stat='count', common_norm=False, bins=50, alpha=0.5)\n",
    "\n",
    "  # Add labels and title\n",
    "  plt.xlabel('Response Length')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.title('Comparison of Output Lengths')\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inlens_to_buckets(inlens):\n",
    "  regions = [0, 25, 100, 250, 500, 1000, 2000]\n",
    "  histogram, _ = np.histogram(inlens, bins=regions)\n",
    "  return [x / sum(histogram) for x in histogram]\n",
    "\n",
    "def lens_to_buckets(inlens, outlens):\n",
    "  regions = [0, 25, 100, 250, 500, 1000, 2000]\n",
    "  histogram, _, _ = np.histogram2d(inlens, outlens, bins=(regions, regions))\n",
    "\n",
    "  num_datapoints = sum([sum(x) for x in histogram])\n",
    "  histogram = histogram.T\n",
    "  histogram = histogram[::-1]\n",
    "  for i in range(len(histogram)):\n",
    "    for j in range(len(histogram[i])):\n",
    "      histogram[i][j] = histogram[i][j] / num_datapoints\n",
    "  return histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00054437 0.         0.         0.         0.         0.        ]\n",
      " [0.01469788 0.03701688 0.00653239 0.01415351 0.01524224 0.01034295]\n",
      " [0.05606968 0.06151334 0.02123027 0.044638   0.0413718  0.03864997]\n",
      " [0.06042461 0.04191617 0.03701688 0.03647251 0.04735983 0.02721829]\n",
      " [0.07294502 0.07076756 0.04354927 0.03375068 0.01850844 0.01415351]\n",
      " [0.06096897 0.03048449 0.01741971 0.01143168 0.00653239 0.00707676]]\n"
     ]
    }
   ],
   "source": [
    "# Arena 1M Conversations\n",
    "arena1m_dataset = get_dataset(\"lmsys/lmsys-chat-1m\")\n",
    "arena1m_token_dataset = get_token_dataset(arena1m_dataset)\n",
    "arena1m_inlens, arena1m_outlens = get_chat_lens(arena1m_token_dataset)\n",
    "print_distribution(arena1m_inlens, arena1m_outlens)\n",
    "display_distribution(arena1m_inlens, arena1m_outlens)\n",
    "\n",
    "# For input+output:\n",
    "arena1m_distribution = lens_to_buckets(arena1m_inlens, arena1m_outlens)\n",
    "\n",
    "# For input-only:\n",
    "arena1m_inlen_distribution = inlens_to_buckets(arena1m_inlens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# Convert max throughput profiling to a mapping from request size to load\n",
    "def tputs_to_loads_2d(max_tputs: List[List[float]]):\n",
    "    loads = []\n",
    "    for i in range(len(max_tputs)):\n",
    "        loads.append([])\n",
    "        for j in range(len(max_tputs[0])):\n",
    "            load = 1 / max_tputs[i][j]\n",
    "            loads[-1].append(load)\n",
    "    return loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_experiment_results(results, solver_labels, ilp_result):\n",
    "  df = pd.DataFrame(results)\n",
    "  df.fillna(0, inplace=True)\n",
    "\n",
    "  # Add the last column filled with zeros\n",
    "  df['Savings'] = [str(round(((x[\"cost\"] - ilp_result[\"cost\"])/x[\"cost\"]) * 100, 2)) + \"%\" for x in results]\n",
    "\n",
    "  # Ensure the 'cost' column is second to last and 'LastColumn' is last, this step might need adjustment based on actual GPU types\n",
    "  # Assuming we don't know all GPU types in advance, let's dynamically sort columns except for 'cost' and 'LastColumn'\n",
    "  gpu_columns = [col for col in df.columns if col not in ['cost', 'Savings']]\n",
    "  sorted_columns = gpu_columns + ['cost', 'Savings']\n",
    "  df = df[sorted_columns]\n",
    "  df.index = solver_labels\n",
    "\n",
    "  # Display the table\n",
    "  print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ILP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input+output solver\n",
    "\n",
    "import pulp\n",
    "from pulp import LpVariable, LpProblem, LpMinimize, LpInteger\n",
    "\n",
    "def run_ILP_solver(workload_distribution, overall_rate, slice_factor, gpu_info, logs=False):\n",
    "    # Multiply overall rate across distribution.\n",
    "    request_rate_histogram = []\n",
    "    for i in range(len(workload_distribution)):\n",
    "        request_rate_histogram.append([])\n",
    "        for j in range(len(workload_distribution[0])):\n",
    "            request_rate_histogram[-1].append(workload_distribution[i][j] * overall_rate)\n",
    "\n",
    "    # Conver the profiled max throughputs into mapping from request size to load\n",
    "    for gpu in gpu_info:\n",
    "        gpu_info[gpu][\"loads\"] = tputs_to_loads_2d(gpu_info[gpu][\"tputs\"])\n",
    "\n",
    "    gpu_types = list(gpu_info.keys())\n",
    "    cost_vector = [gpu_info[gpu][\"cost\"] for gpu in gpu_types]\n",
    "\n",
    "    # Create slices, which is a single dimension.\n",
    "    slices = []\n",
    "    for i in range(len(request_rate_histogram)):\n",
    "      for j in range(len(request_rate_histogram[i])):\n",
    "        for _ in range(slice_factor):\n",
    "            slices.append(request_rate_histogram[i][j] / slice_factor)\n",
    "    \n",
    "    # Create slice-to-load mapping, which is a single dimension.\n",
    "    for gpu in gpu_types:\n",
    "        slice_loads = []\n",
    "        for i in range(len(gpu_info[gpu][\"loads\"])):\n",
    "            for j in range(len(gpu_info[gpu][\"loads\"][i])):\n",
    "                for _ in range(slice_factor):\n",
    "                    slice_loads.append(gpu_info[gpu][\"loads\"][i][j])\n",
    "        assert len(slices) == len(slice_loads)\n",
    "        gpu_info[gpu][\"slice_loads\"] = slice_loads\n",
    "\n",
    "\n",
    "    # Decision matrix value is binary. The slice is assigned to a GPU, or it isn't.\n",
    "    matrix_rows = len(slices)\n",
    "    matrix_cols = len(gpu_types)\n",
    "\n",
    "    # Vector value is non-negative integer of how many of each GPU type are needed\n",
    "    vector_length = matrix_cols\n",
    "\n",
    "    decision_matrix = [[LpVariable(f\"x_{i}_{j}\", cat=LpInteger, lowBound=0, upBound=1) for j in range(matrix_cols)] for i in range(matrix_rows)]\n",
    "    decision_vector = [LpVariable(f\"y_{i}\", cat=LpInteger, lowBound=0) for i in range(vector_length)]\n",
    "\n",
    "    # Objective: minimize cost\n",
    "    problem = LpProblem(\"GpuAllocation\", LpMinimize)\n",
    "    problem += pulp.lpSum([decision_vector[i] * cost_vector[i] for i in range(len(decision_vector))])\n",
    "\n",
    "    # C1: Each row of decision matrix must sum to exactly 1 (ie, each slice assigned to one GPU)\n",
    "    for i in range(len(decision_matrix)):\n",
    "        problem += pulp.lpSum(decision_matrix[i]) == 1\n",
    "\n",
    "    # C2: Load of column of decision matrix must fit in decision vector capacity\n",
    "    for j in range(len(decision_matrix[0])):\n",
    "        # j is idx of GPU type, i is slice\n",
    "        problem += pulp.lpSum([decision_matrix[i][j] * gpu_info[gpu_types[j]][\"slice_loads\"][i] * slices[i] for i in range(len(decision_matrix))]) <= decision_vector[j]\n",
    "        # problem += pulp.lpSum([decision_matrix[i][j] * norm_coefficients[j][i] * request_rate_histogram[i] for i in range(len(decision_matrix))]) <= decision_vector[j]\n",
    "\n",
    "    # Solve the problem\n",
    "    problem.solve(pulp.PULP_CBC_CMD(msg=0))\n",
    "\n",
    "    # Print the results\n",
    "    # print(\"Status:\", pulp.LpStatus[problem.status])\n",
    "    if logs:\n",
    "        print(f'Decision Matrix:')\n",
    "        for row in decision_matrix:\n",
    "            print([var.value() for var in row])\n",
    "        print(f'Decision Vector:')\n",
    "        print(f'{[var.value() for var in decision_vector]}')\n",
    "\n",
    "    if pulp.LpStatus[problem.status] != 'Optimal':\n",
    "        return None\n",
    "    \n",
    "    solution_dict = {}\n",
    "    for i in range(len(decision_vector)):\n",
    "        solution_dict[gpu_types[i]] = decision_vector[i].value()\n",
    "\n",
    "    total_cost = 0\n",
    "    for gpu in solution_dict:\n",
    "        total_cost += solution_dict[gpu] * gpu_info[gpu][\"cost\"]\n",
    "    solution_dict[\"cost\"] = total_cost\n",
    "    \n",
    "    return solution_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Accelerator Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def run_single_accelerator_solver(workload_distribution, overall_rate, gpu_type, gpu_info):\n",
    "  # Multiply overall rate across distribution.\n",
    "  request_rate_histogram = []\n",
    "  for i in range(len(workload_distribution)):\n",
    "      request_rate_histogram.append([])\n",
    "      for j in range(len(workload_distribution[0])):\n",
    "          request_rate_histogram[-1].append(workload_distribution[i][j] * overall_rate)\n",
    "\n",
    "  # Get the request size to load mapping for this specific GPU.\n",
    "  loads = tputs_to_loads_2d(gpu_info[gpu_type][\"tputs\"])\n",
    "\n",
    "  # Compute aggregate load.\n",
    "  aggregate_load = 0\n",
    "  for i in range(len(request_rate_histogram)):\n",
    "     for j in range(len(request_rate_histogram[i])):\n",
    "        aggregate_load += request_rate_histogram[i][j] * loads[i][j]\n",
    "  \n",
    "  num_gpus = math.ceil(aggregate_load)\n",
    "  return { gpu_type : num_gpus, \"cost\" : num_gpus * gpu_info[gpu_type][\"cost\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Creation\n",
    "Choose the parameters of the experiment setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           A10G  A100   cost Savings\n",
      "ILP         1.0   2.0   8.35    0.0%\n",
      "A100-only   0.0   3.0  11.01  24.16%\n",
      "A10G-only   9.0   0.0   9.09   8.14%\n"
     ]
    }
   ],
   "source": [
    "gpu_info_llama7b = {\n",
    "   \"A10G\" : {\n",
    "      \"cost\": 1.01,\n",
    "      \"tputs\": [\n",
    "                  [33.96,12.35,3.98,1.46,0.44,0.13],\n",
    "                  [25.49,7.85,2.99,1.12,0.39,0.11],\n",
    "                  [11.47,5.5,2.15,0.86,0.32,0.11],\n",
    "                  [6.16,2.62,1.32,0.6,0.26,0.11],\n",
    "                  [3.1,1.51,0.77,0.38,0.17,0.07],\n",
    "                  [1.61,0.81,0.42,0.22,0.11,0.04]\n",
    "               ],\n",
    "   },\n",
    "   \"A100\" : {\n",
    "      \"cost\": 3.67,\n",
    "      \"tputs\": [\n",
    "                  [46.7,21.65,10.38,4.68,1.7,0.51],\n",
    "                  [44.48,21.41,9.77,3.97,1.57,0.45],\n",
    "                  [33.7,17.13,7.58,3.29,1.28,0.52],\n",
    "                  [18.59,9.19,4.74,2.42,1.04,0.47],\n",
    "                  [9.85,5.56,3.01,1.62,0.73,0.3],\n",
    "                  [5.22,3.08,1.72,0.97,0.48,0.2],\n",
    "               ],\n",
    "   }\n",
    "}\n",
    "\n",
    "workload_distribution = arena1m_distribution\n",
    "\n",
    "overall_rate = 4\n",
    "\n",
    "slice_factor = 1\n",
    "\n",
    "ilp_result = run_ILP_solver(workload_distribution=workload_distribution, overall_rate=overall_rate, slice_factor=slice_factor, gpu_info=gpu_info_llama7b, logs=False)\n",
    "a100_result = run_single_accelerator_solver(workload_distribution=workload_distribution, overall_rate=overall_rate, gpu_type=\"A100\", gpu_info=gpu_info_llama7b)\n",
    "a10g_result = run_single_accelerator_solver(workload_distribution=workload_distribution, overall_rate=overall_rate, gpu_type=\"A10G\", gpu_info=gpu_info_llama7b)\n",
    "\n",
    "results = [ilp_result, a100_result, a10g_result]\n",
    "solver_labels = [\"ILP\", \"A100-only\", \"A10G-only\"]\n",
    "display_experiment_results(results, solver_labels, ilp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A10G': 10.0, 'A100': 0.0, 'cost': 10.1}\n"
     ]
    }
   ],
   "source": [
    "gpu_info_example = {\n",
    "   \"A10G\" : {\n",
    "      \"cost\": 1.01,\n",
    "      \"tputs\": [[5, 1], \n",
    "                [10,5]],\n",
    "   },\n",
    "   \"A100\" : {\n",
    "      \"cost\": 3.67,\n",
    "      \"tputs\": [[20, 2],\n",
    "                [50, 20]],\n",
    "   }\n",
    "}\n",
    "\n",
    "workload_distribution = [[0.25, 0.5],\n",
    "                         [0.25, 0.25]]\n",
    "\n",
    "overall_rate = 16\n",
    "\n",
    "slice_factor = 1\n",
    "\n",
    "print(run_ILP_solver(workload_distribution=workload_distribution, overall_rate=overall_rate, slice_factor=slice_factor, gpu_info=gpu_info_example, logs=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
